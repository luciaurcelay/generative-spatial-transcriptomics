{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal Generative Modeling of Spatial Transcriptomics using VAEs and GNNs"
      ],
      "metadata": {
        "id": "3alsLaS8ez9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build the model architecture, integrating spatial transcriptomics, WSI images, and metadata, we'll use Graph Neural Networks (GNN) for the spatial transcriptomics encoder, H-Optimus-0 for the WSI encoder, and a simple Fully Connected Network (FCN) or embedding layer for the metadata. Afterward, we'll combine the outputs into a shared latent space and then decode them back to the respective domains."
      ],
      "metadata": {
        "id": "oX0z691NeuzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scanpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A3rQsS0IfODp",
        "outputId": "cd328c30-16d3-4be0-8d28-9734d782650b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rQH-cUO8aNWb",
        "outputId": "d3ca0244-2730-4523-ad9c-d77908f6ef67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import scanpy as sc\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "bc6e5kybazQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SEWDojUap7s",
        "outputId": "162f60ef-1991-47d4-d296-89c084d37f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation"
      ],
      "metadata": {
        "id": "9oMdK0DZfmB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set local directory fro data reading\n",
        "data_dir = \"/content/drive/My Drive/Projects/Synthetic Spatial Transcriptomics/hest_data_xenium\"\n",
        "st_dir = data_dir + '/st'  # Spatial transcriptomics files (h5ad)\n",
        "metadata_dir =  data_dir + '/metadata'  # Metadata files (json)\n",
        "thumbnails_dir = data_dir + '/thumbnails'  # Histology images (jpeg)"
      ],
      "metadata": {
        "id": "BHZ0mC7_awyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get all file identifiers from a directory\n",
        "def get_file_identifiers(folder_path, extension):\n",
        "    # List all files in the folder and filter by extension\n",
        "    files = [f for f in os.listdir(folder_path) if f.endswith(extension)]\n",
        "    # Extract the identifiers\n",
        "    identifiers = [f.split('.')[0] for f in files]\n",
        "    return identifiers\n",
        "\n",
        "# Get identifiers for each data type\n",
        "st_identifiers = get_file_identifiers(st_dir, '.h5ad')\n",
        "wsi_identifiers = get_file_identifiers(thumbnails_dir, '.jpeg')\n",
        "metadata_identifiers = get_file_identifiers(metadata_dir, '.json')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "51FAsn1xgreV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the paths dynamically based on the identifiers\n",
        "def generate_paths(identifiers, st_folder, wsi_folder, metadata_folder):\n",
        "    st_paths = [os.path.join(st_folder, f\"{identifier}.h5ad\") for identifier in identifiers]\n",
        "    wsi_paths = [os.path.join(wsi_folder, f\"{identifier}.jpeg\") for identifier in identifiers]\n",
        "    metadata_paths = [os.path.join(metadata_folder, f\"{identifier}.json\") for identifier in identifiers]\n",
        "\n",
        "    return st_paths, wsi_paths, metadata_paths\n",
        "\n",
        "# Example usage\n",
        "st_folder = 'path_to_st_folder'\n",
        "wsi_folder = 'path_to_wsi_folder'\n",
        "metadata_folder = 'path_to_metadata_folder'\n",
        "\n",
        "# Generate paths\n",
        "st_paths, wsi_paths, metadata_paths = generate_paths(st_identifiers, st_dir, thumbnails_dir, metadata_dir)\n"
      ],
      "metadata": {
        "id": "9KQeF_T9iJ4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess metadata to create one-hot encoding maps\n",
        "def preprocess_metadata(metadata_paths):\n",
        "    # Use sets to collect unique values\n",
        "    unique_values = {\"organ\": set(), \"disease_state\": set(), \"species\": set(), \"tissue\": set()}\n",
        "\n",
        "    for path in metadata_paths:\n",
        "        with open(path, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "            for key in unique_values.keys():\n",
        "                unique_values[key].add(metadata.get(key, \"unknown\"))\n",
        "\n",
        "    # Create one-hot encoding maps\n",
        "    encoding_maps = {\n",
        "        key: {val: i for i, val in enumerate(sorted(unique_values[key]))}\n",
        "        for key in unique_values\n",
        "    }\n",
        "    return encoding_maps\n",
        "\n",
        "# One-hot encode metadata values\n",
        "def load_metadata(metadata_path, encoding_maps):\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # One-hot encode fields and concatenate\n",
        "    onehot_tensors = [\n",
        "        torch.nn.functional.one_hot(\n",
        "            torch.tensor(encoding_maps[key].get(metadata.get(key, \"unknown\"), 0)),\n",
        "            num_classes=len(encoding_maps[key])\n",
        "        )\n",
        "        for key in encoding_maps.keys()\n",
        "    ]\n",
        "    return torch.cat(onehot_tensors, dim=0)\n"
      ],
      "metadata": {
        "id": "YO2Fl0T8oEvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    st_data_list, wsi_images, metadata_list, edge_index_list = zip(*batch)\n",
        "\n",
        "    # Determine maximum sizes for padding\n",
        "    max_spots = max(st.size(0) for st in st_data_list)\n",
        "    max_genes = max(st.size(1) for st in st_data_list)\n",
        "\n",
        "    # Pad each spatial transcriptomics tensor\n",
        "    padded_st_data = [pad_tensor(st, max_spots, max_genes) for st in st_data_list]\n",
        "\n",
        "    # Stack padded tensors into a single tensor\n",
        "    padded_st_data_tensor = torch.stack(padded_st_data)\n",
        "\n",
        "    # Ensure WSI images and metadata are handled correctly\n",
        "    wsi_images_tensor = torch.stack([transforms.ToTensor()(img) for img in wsi_images])  # Assuming images are PIL\n",
        "    metadata_tensor = torch.cat(metadata_list)  # Concatenate metadata tensors\n",
        "\n",
        "    return padded_st_data_tensor, wsi_images_tensor, metadata_tensor, edge_index_list\n",
        "\n",
        "\n",
        "def compute_edge_index(spatial_coords):\n",
        "    from scipy.spatial import KDTree\n",
        "\n",
        "    tree = KDTree(spatial_coords)\n",
        "    edges = tree.query_pairs(r=1.0)  # TBD: Adjust radius based on data scale\n",
        "    edge_index = torch.tensor(list(edges), dtype=torch.long).t().contiguous()  # Convert to tensor\n",
        "    return edge_index\n",
        "\n",
        "def pad_tensor(tensor, max_spots, max_genes):\n",
        "    # Pad spots\n",
        "    pad_spots = max_spots - tensor.size(0)\n",
        "    if pad_spots > 0:\n",
        "        padding_spots = torch.zeros(pad_spots, tensor.size(1), dtype=tensor.dtype, device=tensor.device)\n",
        "        tensor = torch.cat([tensor, padding_spots], dim=0)\n",
        "\n",
        "    # Pad genes (if necessary)\n",
        "    if tensor.size(1) < max_genes:\n",
        "        padding_genes = torch.zeros(tensor.size(0), max_genes - tensor.size(1), dtype=tensor.dtype, device=tensor.device)\n",
        "        tensor = torch.cat([tensor, padding_genes], dim=1)\n",
        "\n",
        "    return tensor\n",
        "\n",
        "class SpatialTranscriptomicsDataset(Dataset):\n",
        "    def __init__(self, st_files, wsi_files, metadata_files, encoding_maps):\n",
        "        self.st_files = st_files\n",
        "        self.wsi_files = wsi_files\n",
        "        self.metadata_files = metadata_files\n",
        "        self.encoding_maps = encoding_maps\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.st_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load spatial transcriptomics data\n",
        "        st_data = sc.read_h5ad(self.st_files[idx])\n",
        "\n",
        "        # Ensure dense format\n",
        "        st_data_dense = (\n",
        "            st_data.X if isinstance(st_data.X, np.ndarray) else st_data.X.todense()\n",
        "        )\n",
        "        st_data_tensor = torch.tensor(st_data_dense, dtype=torch.float32)\n",
        "\n",
        "        # Load spatial coordinates and compute edge index\n",
        "        spatial_coords = st_data.obsm['spatial']\n",
        "        edge_index = compute_edge_index(spatial_coords)\n",
        "\n",
        "        # Load WSI image and transform\n",
        "        wsi_image = Image.open(self.wsi_files[idx]).convert(\"RGB\")\n",
        "        wsi_image = wsi_image.resize((224, 224))\n",
        "\n",
        "        # Load metadata\n",
        "        metadata_path = self.metadata_files[idx]\n",
        "        metadata_onehot = load_metadata(metadata_path, self.encoding_maps)\n",
        "\n",
        "        return st_data_tensor, wsi_image, metadata_onehot, edge_index\n",
        "\n",
        "\n",
        "# Initialize dataset\n",
        "encoding_maps = preprocess_metadata(metadata_paths)\n",
        "dataset = SpatialTranscriptomicsDataset(st_paths, wsi_paths, metadata_paths, encoding_maps)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "RHUYHe0voIIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Models"
      ],
      "metadata": {
        "id": "0OFIQ0ekjgNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WSI Encoder (using Bioptimus/H-Optimus-0)"
      ],
      "metadata": {
        "id": "pgqZZxRijpRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Define the WSI encoder\n",
        "class WSIEncoder(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WSIEncoder, self).__init__()\n",
        "        # Load H-Optimus from Huggingface via timm\n",
        "        self.model = timm.create_model(\n",
        "            \"hf-hub:bioptimus/H-optimus-0\",\n",
        "            pretrained=True,\n",
        "            init_values=1e-5,\n",
        "            dynamic_img_size=False\n",
        "        )\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=(0.707223, 0.578729, 0.703617),\n",
        "                std=(0.211883, 0.230117, 0.177517)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply transforms\n",
        "        transformed_x = torch.stack([self.transform(img) for img in x])\n",
        "\n",
        "        # Move input to CUDA\n",
        "        transformed_x = transformed_x.to(\"cuda\")\n",
        "\n",
        "        # Mixed precision inference with H-Optimus\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "            with torch.inference_mode():\n",
        "                features = self.model(transformed_x)\n",
        "\n",
        "        return features\n",
        "\n",
        "# Initialize WSI encoder\n",
        "wsi_encoder = WSIEncoder().to(\"cuda\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-qfC9BSWWAW",
        "outputId": "b4f1c963-7d42-4236-a203-8977589894e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spatial Transcriptomics Encoder (Graph Neural Network)"
      ],
      "metadata": {
        "id": "xSreF1N1jtPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be done:\n",
        "\n",
        "- Normalize gene expression data\n",
        "- Pre-define a list of genes (union of all genes in HEST-1K dataset) and map gene expression from each sample to the list, this will ensure that genes are always inputted in the same order to the model"
      ],
      "metadata": {
        "id": "w3Nmk7BoZ8Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class STEncoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(STEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = torch.relu(self.conv1(x, edge_index))  # First convolution\n",
        "        x = torch.relu(self.conv2(x, edge_index))  # Second convolution\n",
        "        return x\n",
        "\n",
        "# Example usage (spatial transcriptomics as graph)\n",
        "input_dim = 541  # Number of genes (input dimension)\n",
        "hidden_dim = 128  # Hidden layer dimension\n",
        "output_dim = 64  # Latent space dimension\n",
        "st_encoder = STEncoder(input_dim, hidden_dim, output_dim)\n"
      ],
      "metadata": {
        "id": "SjmFaNX1jxEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metadata Encoder"
      ],
      "metadata": {
        "id": "vf5Ezks5j1Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetadataEncoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, output_dim):\n",
        "        super(MetadataEncoder, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, embedding_dim)\n",
        "        self.fc2 = torch.nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "metadata_encoder = MetadataEncoder(input_dim=4, embedding_dim=32, output_dim=16)  # Example dims\n"
      ],
      "metadata": {
        "id": "4DzJJQX1j3cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Space and Decoder"
      ],
      "metadata": {
        "id": "z5AgDG0gkOZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be done:\n",
        "- Implement decoder to reconstruct WSI and Spatial Transcriptomic data"
      ],
      "metadata": {
        "id": "e_w7cBR1XRvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(torch.nn.Module):\n",
        "    def __init__(self, st_encoder, wsi_encoder, metadata_encoder, latent_dim=64):\n",
        "        super(VAE, self).__init__()\n",
        "        self.st_encoder = st_encoder\n",
        "        self.wsi_encoder = wsi_encoder\n",
        "        self.metadata_encoder = metadata_encoder\n",
        "\n",
        "        # Latent space\n",
        "        self.latent_dim = latent_dim\n",
        "        self.fc1 = torch.nn.Linear(64 + 768 + 16, latent_dim)  # Combine all modalities\n",
        "        self.fc2 = torch.nn.Linear(latent_dim, 64)  # Decoder layer to reconstruct spatial transcriptomics\n",
        "\n",
        "    def forward(self, st_data, wsi_data, metadata, edge_index):\n",
        "        # Encode spatial transcriptomics (GNN)\n",
        "        st_embedding = self.st_encoder(st_data, edge_index)\n",
        "\n",
        "        # Encode histopathology images (H-Optimus)\n",
        "        wsi_embedding = self.wsi_encoder(wsi_data)  # Returns features for each WSI\n",
        "\n",
        "        # Encode metadata\n",
        "        metadata_embedding = self.metadata_encoder(metadata)\n",
        "\n",
        "        # Combine embeddings from all modalities\n",
        "        combined_embedding = torch.cat([st_embedding.mean(dim=0), wsi_embedding.mean(dim=0), metadata_embedding], dim=-1)\n",
        "\n",
        "        # Latent space\n",
        "        latent_space = torch.relu(self.fc1(combined_embedding))\n",
        "        latent_space = self.fc2(latent_space)\n",
        "\n",
        "        return latent_space  # Return latent space instead of reconstructed ST directly\n",
        "\n",
        "# Initialize the VAE\n",
        "vae = VAE(st_encoder, wsi_encoder, metadata_encoder, latent_dim=64).to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "kIZHDGZFXYJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "XMueaUswkWTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def adjust_edge_index(edge_index, current_size):\n",
        "    # Ensure edge_index is a tensor\n",
        "    if isinstance(edge_index, np.ndarray):\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "    adjusted_edges = []\n",
        "\n",
        "    # Transpose the edge index for easier access\n",
        "    for src, dst in edge_index.t().tolist():\n",
        "        if src < current_size and dst < current_size:  # Only keep valid edges\n",
        "            adjusted_edges.append((src, dst))\n",
        "\n",
        "    # Convert back to tensor and ensure it's on the right device\n",
        "    if adjusted_edges:\n",
        "        adjusted_edge_index = torch.tensor(adjusted_edges, dtype=torch.long).t().contiguous()\n",
        "    else:\n",
        "        # If no edges are valid, create an empty edge index\n",
        "        adjusted_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "\n",
        "    return adjusted_edge_index.to(edge_index.device)  # Use the device of the original edge index\n",
        "\n",
        "# Set up optimizer and loss function\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n",
        "reconstruction_loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    vae.train()\n",
        "    for batch in dataloader:\n",
        "        st_data_tensor, wsi_images_tensor, metadata_tensor, edge_index_list = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Adjust edge indices based on current sizes before passing them into VAE\n",
        "        adjusted_edge_indices = [adjust_edge_index(edge_index.numpy(), st.size(0)) for edge_index, st in zip(edge_index_list, st_data_tensor)]\n",
        "\n",
        "        # Ensure all adjusted_edge_indices are tensors and have correct shape\n",
        "        adjusted_edge_indices = [edge_index.to(\"cuda\") for edge_index in adjusted_edge_indices]\n",
        "\n",
        "        print(\"ST Data Shape:\", st_data_tensor.shape)\n",
        "        print(\"WSI Image Shape:\", wsi_images_tensor.shape)\n",
        "        print(\"Metadata Shape:\", metadata_tensor.shape)\n",
        "        print(\"Adjusted Edge Indices Shape:\", [edge_index.shape for edge_index in adjusted_edge_indices])\n",
        "\n",
        "        reconstructed_st = vae(\n",
        "            st_data_tensor.to(\"cuda\"),\n",
        "            wsi_images_tensor.to(\"cuda\"),\n",
        "            metadata_tensor.to(\"cuda\"),\n",
        "            adjusted_edge_indices\n",
        "        )\n",
        "\n",
        "        loss = reconstruction_loss_fn(reconstructed_st, st_data_tensor.to(\"cuda\"))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "IsH2O2FYX8rA",
        "outputId": "8a813926-6549-4c8f-be29-1d8f911f12ca",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ST Data Shape: torch.Size([8, 11845, 10017])\n",
            "WSI Image Shape: torch.Size([8, 3, 224, 224])\n",
            "Metadata Shape: torch.Size([296])\n",
            "Adjusted Edge Indices Shape: [torch.Size([2, 0]), torch.Size([2, 0]), torch.Size([2, 0]), torch.Size([2, 0]), torch.Size([2, 0]), torch.Size([2, 0]), torch.Size([2, 0]), torch.Size([2, 0])]\n"
          ]
        }
      ]
    }
  ]
}
